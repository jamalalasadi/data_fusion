(A Fairness-Aware Fusion Framework for Multimodal Cyberbullying Detection)




Data Fusion
Recent reports of bias in multimedia algorithms  (e.g., lesser accuracy of face detection for women and persons of color) have underscored the urgent need to devise approaches
 which work equally well for different demographic groups.  Hence, we posit that ensuring fairness in multimodal cyber bullying detectors (e.g., equal performance irrespective of the
 gender of the victim) is an important research challenge. We propose a fairness-aware fusion framework that ensures that both fairness and accuracy remain important considerations
 when combining data coming from multiple modalities. In this Bayesian fusion framework, the inputs coming from  different modalities are combined in a way that is cognizant
 of the different confidence levels associated with each feature  and the interdependencies between features. Specifically, this framework assigns weights to different modalities not just
 based on accuracy but also their fairness. Results of applying  the framework on a multimodal (visual + text) cyberbullying detection problem demonstrate the value of the proposed
 framework in ensuring both accuracy and fairness
